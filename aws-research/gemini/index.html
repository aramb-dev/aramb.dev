<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini AI on AWS EC2 | Self-Hosted AI Research</title>
    <meta name="description"
        content="Build your own cloud AI development environment using AWS EC2, Ollama for running models, and Code-Server">
    <meta property="og:url" content="https://aramb.dev/aws-research/gemini">
    <meta property="og:type" content="website">
    <meta property="og:title" content="Gemini AI on AWS EC2 | Self-Hosted AI Research">
    <meta property="og:description"
        content="Build your own cloud AI development environment using AWS EC2, Ollama for running models, and Code-Server">
    <meta property="og:image" content="/assets/img/Abdur-Rahmān Bilāl - Social Share.png">
    <meta name="twitter:card" content="summary_large_image">
    <meta property="twitter:domain" content="aramb.dev">
    <meta property="twitter:url" content="https://aramb.dev/aws-research/gemini">
    <meta name="twitter:title" content="Gemini AI on AWS EC2 | Self-Hosted AI Research">
    <meta name="twitter:description"
        content="Build your own cloud AI development environment using AWS EC2, Ollama for running models, and Code-Server">
    <meta name="twitter:image" content="/assets/img/Abdur-Rahmān Bilāl - Social Share.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&display=swap"
        rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <style>
        body {
            font-family: 'IBM Plex Sans', system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        }

        pre {
            background-color: #282c34;
            color: #abb2bf;
            padding: 1rem;
            border-radius: 0.5rem;
            overflow-x: auto;
            margin: 1rem 0;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.875rem;
        }

        code {
            background-color: #f0f0f0;
            padding: 0.125rem 0.25rem;
            border-radius: 0.25rem;
            font-family: 'Courier New', Courier, monospace;
            font-size: 0.875rem;
        }

        pre code {
            background-color: transparent;
            padding: 0;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
        }

        .note {
            @apply bg-blue-50 border-l-4 border-blue-500 p-4 my-4;
        }

        .recommendation {
            @apply bg-green-50 border-l-4 border-green-500 p-4 my-4;
        }

        .warning {
            @apply bg-yellow-50 border-l-4 border-yellow-500 p-4 my-4;
        }
    </style>
    <link rel="icon" type="image/png" href="/assets/favicon/favicon-96x96.png" sizes="96x96" />
    <link rel="icon" type="image/svg+xml" href="/assets/favicon/favicon.svg" />
    <link rel="shortcut icon" href="/assets/favicon/favicon.ico" />
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png" />
    <meta name="apple-mobile-web-app-title" content="Abdur-Rahmān Bilāl" />
    <link rel="manifest" href="/assets/favicon/site.webmanifest" />

    <!-- Add smooth scrolling to the entire site -->
    <link rel="stylesheet" href="/assets/css/smooth-scroll.css">
    <script src="/assets/js/smooth-scroll.js" defer></script>
    
    <!-- Back to top button -->
    <link rel="stylesheet" href="/assets/css/back-to-top.css">
    <script src="/assets/js/back-to-top.js" defer></script>
</head>

<body class="bg-gray-50">
    <header class="bg-gradient-to-l to-green-600 from-green-800 text-white py-16">
        <div class="container mx-auto px-4 max-w-4xl">
            <div class="flex items-center gap-4 mb-4">
                <a href="/" class="text-white hover:underline flex items-center gap-2">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
                        <path
                            d="M10.707 2.293a1 1 0 00-1.414 0l-7 7a1 1 0 001.414 1.414L4 10.414V17a1 1 0 001 1h2a1 1 0 001-1v-2a1 1 0 011-1h2a1 1 0 011 1v2a1 1 0 001 1h2a1 1 0 001-1v-6.586l.293.293a1 1 0 001.414-1.414l-7-7z" />
                    </svg>
                    Home
                </a>
                <span class="text-gray-200">/</span>
                <a href="/aws-research/" class="text-white hover:underline">AWS Research</a>
                <span class="text-gray-200">/</span>
                <span>Gemini</span>
            </div>
            <div class="text-center">
                <h1 class="text-4xl font-bold mb-2">Build Your Own Cloud AI Dev Box</h1>
                <p class="text-xl">A Guide to Self-Hosting on AWS EC2</p>
            </div>
        </div>
    </header>

    <main class="container mx-auto px-4 max-w-4xl py-12">
        <div class="bg-white rounded-lg shadow-md p-8 mb-12">
            <p class="mb-4">Ditch the SaaS fees and run your own powerful AI development environment on AWS. Get the
                convenience of cloud
                AI tools like Replit Ghostwriter or Cursor AI, but with full control, local models, and optimized costs.
            </p>

            <p>This guide details how to set up a personal, cost-effective AI development server using AWS EC2, Ollama
                for
                running models, Code-Server for a browser-based VS Code experience, and Nginx for secure access.</p>
        </div>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Why Self-Host Your AI Assistant?</h2>
            </div>
            <div class="p-6">
                <ul class="list-disc pl-5 space-y-2 text-gray-700">
                    <li><strong>Control & Customization:</strong> Run any model compatible with Ollama or LocalGPT.
                        Tweak
                        settings, manage your data privately, and choose your hardware.</li>
                    <li><strong>Cost Savings:</strong> Avoid recurring monthly SaaS fees. Pay only for the AWS resources
                        you
                        consume, significantly reduced by using Spot Instances and automated shutdowns during idle
                        times.</li>
                    <li><strong>Performance:</strong> Leverage powerful AWS GPU instances (like the G5 family) for much
                        faster
                        AI model inference compared to typical local machines.</li>
                    <li><strong>Learning Opportunity:</strong> Gain valuable hands-on experience with cloud
                        infrastructure (EC2,
                        IAM, Nginx, Docker) and AI model deployment practices.</li>
                </ul>
            </div>
        </section>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Choosing Your AWS Hardware: EC2 Instance Guide</h2>
            </div>
            <div class="p-6">
                <h3 class="text-xl font-semibold mb-4 text-green-700">The Importance of GPUs (Especially VRAM)</h3>
                <p class="text-gray-700 mb-4">
                    Running Large Language Models (LLMs) effectively is often limited by the amount of Video RAM (VRAM)
                    available
                    on the GPU. Model size (billions of parameters) directly impacts VRAM usage. Techniques like
                    <strong>quantization</strong> (reducing the numerical precision of model weights) can significantly
                    lower
                    VRAM needs, but overly aggressive quantization might reduce model quality.
                </p>
                <p class="text-gray-700 mb-4">
                    For developer tasks (code generation, complex instructions), maintaining model quality is key. Aim
                    for enough
                    VRAM to run your desired models with at least 4-bit or 5-bit quantization (e.g., Q4_K_M GGUF) for a
                    good
                    balance.
                </p>
                <p class="text-gray-700 mb-2"><strong>Estimated VRAM Needs (Quantized Models):</strong></p>
                <ul class="list-disc pl-5 mb-4 text-gray-700">
                    <li>~7-13B models (e.g., Llama 3 8B, Mistral 7B, DeepSeek Coder 6.7B, Phi-3-mini): ~4-12 GB (Q4)
                    </li>
                    <li>Mixtral 8x7B: ~27-30 GB (Q4), ~24GB (3.5bpw EXL2)</li>
                    <li>Llama 3 70B: ~40-50 GB (Q4/Q5), ~75GB (Q8), ~140GB+ (FP16)</li>
                </ul>
                <div class="bg-blue-50 border-l-4 border-blue-500 p-4 my-4">
                    <p class="text-blue-700">
                        <strong>Note:</strong> Running large models like Llama 3 70B on GPUs with insufficient VRAM
                        (e.g., 24GB)
                        requires offloading layers to system RAM, which drastically slows down inference speed.
                    </p>
                </div>

                <h3 class="text-xl font-semibold mb-4 mt-8 text-green-700">G5 Instances Recommended</h3>
                <p class="text-gray-700 mb-4">
                    AWS EC2 G5 instances, featuring NVIDIA A10G GPUs (24GB VRAM each), are generally the best choice for
                    this
                    workload. They offer significantly better ML inference performance and price/performance compared to
                    the
                    older G4dn instances (NVIDIA T4, 16GB VRAM). The extra VRAM per GPU is also a major advantage.
                </p>

                <div class="bg-green-50 border-l-4 border-green-500 p-4 my-4">
                    <p class="text-green-700">
                        <strong>Recommendation:</strong> Prioritize G5 instances. Use G4dn only if budget is extremely
                        tight and
                        only small models are needed.
                    </p>
                </div>

                <h3 class="text-xl font-semibold mb-4 mt-8 text-green-700">Recommended Instance Types (G5 Family)</h3>
                <div class="overflow-x-auto">
                    <table class="w-full border-collapse">
                        <thead>
                            <tr class="bg-green-100">
                                <th class="border border-green-300 px-4 py-2 text-left text-green-800">Instance Type
                                </th>
                                <th class="border border-green-300 px-4 py-2 text-left text-green-800">GPU & VRAM</th>
                                <th class="border border-green-300 px-4 py-2 text-left text-green-800">vCPUs</th>
                                <th class="border border-green-300 px-4 py-2 text-left text-green-800">System RAM</th>
                                <th class="border border-green-300 px-4 py-2 text-left text-green-800">Suitability</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td class="border border-gray-300 px-4 py-2"><code>g5.xlarge</code></td>
                                <td class="border border-gray-300 px-4 py-2">1 x A10G (24GB)</td>
                                <td class="border border-gray-300 px-4 py-2">4</td>
                                <td class="border border-gray-300 px-4 py-2">16 GiB</td>
                                <td class="border border-gray-300 px-4 py-2">Good start for models up to ~13B,
                                    potentially Mixtral (low quant/offload). Best cost/capability balance for smaller
                                    models.</td>
                            </tr>
                            <tr class="bg-gray-50">
                                <td class="border border-gray-300 px-4 py-2"><code>g5.2xlarge</code></td>
                                <td class="border border-gray-300 px-4 py-2">1 x A10G (24GB)</td>
                                <td class="border border-gray-300 px-4 py-2">8</td>
                                <td class="border border-gray-300 px-4 py-2">32 GiB</td>
                                <td class="border border-gray-300 px-4 py-2">Same VRAM, more CPU/RAM for smoother
                                    multitasking or minor offloading.</td>
                            </tr>
                            <tr>
                                <td class="border border-gray-300 px-4 py-2"><code>g5.4xlarge</code></td>
                                <td class="border border-gray-300 px-4 py-2">1 x A10G (24GB)</td>
                                <td class="border border-gray-300 px-4 py-2">16</td>
                                <td class="border border-gray-300 px-4 py-2">64 GiB</td>
                                <td class="border border-gray-300 px-4 py-2">More RAM allows more significant layer
                                    offloading for larger models (e.g., Mixtral Q4, Llama 70B Q2/Q3), but expect
                                    performance impact.</td>
                            </tr>
                            <tr class="bg-green-50 font-semibold">
                                <td class="border border-green-300 px-4 py-2"><code>g5.12xlarge</code></td>
                                <td class="border border-green-300 px-4 py-2">4 x A10G (96GB total)</td>
                                <td class="border border-green-300 px-4 py-2">48</td>
                                <td class="border border-green-300 px-4 py-2">192 GiB</td>
                                <td class="border border-green-300 px-4 py-2">Recommended for running Llama 3 70B
                                    (Q4/Q5) efficiently without significant offloading. Also handles Mixtral easily.
                                </td>
                            </tr>
                            <tr>
                                <td class="border border-gray-300 px-4 py-2"><code>g5.24xlarge</code></td>
                                <td class="border border-gray-300 px-4 py-2">4 x A10G (96GB total)</td>
                                <td class="border border-gray-300 px-4 py-2">96</td>
                                <td class="border border-gray-300 px-4 py-2">384 GiB</td>
                                <td class="border border-gray-300 px-4 py-2">More CPU/RAM for demanding workloads or
                                    multiple users/models.</td>
                            </tr>
                            <tr class="bg-gray-50">
                                <td class="border border-gray-300 px-4 py-2"><code>g5.48xlarge</code></td>
                                <td class="border border-gray-300 px-4 py-2">8 x A10G (192GB total)</td>
                                <td class="border border-gray-300 px-4 py-2">192</td>
                                <td class="border border-gray-300 px-4 py-2">768 GiB</td>
                                <td class="border border-gray-300 px-4 py-2">High-end for multiple large models, less
                                    quantization, or very large context windows.</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

                <h3 class="text-xl font-semibold mb-4 mt-8 text-green-700">Spot vs. On-Demand Instances</h3>
                <p class="text-gray-700 mb-4">
                    <strong>Spot Instances</strong> offer huge savings (often 70-90% off On-Demand) by using spare AWS
                    capacity.
                    The catch is AWS can reclaim the instance with a 2-minute warning. For a personal dev server with
                    auto-shutdown, Spot is highly recommended for cost savings.
                </p>
                <p class="text-gray-700">
                    <strong>On-Demand Instances</strong> provide guaranteed availability at a fixed hourly rate,
                    suitable if
                    interruptions are unacceptable.
                </p>
            </div>
        </section>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Step-by-Step Setup Guide</h2>
            </div>
            <div class="p-6">
                <h3 class="text-xl font-semibold mb-4 text-green-700">1. Prerequisites</h3>
                <ul class="list-disc pl-5 space-y-2 text-gray-700 mb-6">
                    <li>Active AWS Account with necessary permissions (EC2, IAM, etc.).</li>
                    <li>(Optional but Recommended) Registered domain name for easy access and SSL.</li>
                    <li>Launched EC2 Instance (choose from recommendations above).</li>
                </ul>

                <h3 class="text-xl font-semibold mb-4 text-green-700">2. Foundation: OS and Drivers</h3>
                <div class="bg-green-50 border-l-4 border-green-500 p-4 my-4">
                    <p class="text-green-700">
                        <strong>Highly Recommended:</strong> Start with an <strong>AWS Deep Learning AMI
                            (DLAMI)</strong> based
                        on Ubuntu (e.g., 22.04 or 24.04). These AMIs come pre-installed with compatible NVIDIA drivers,
                        CUDA
                        toolkit, cuDNN, and often Docker, saving significant setup effort.
                    </p>
                </div>
                <p class="text-gray-700 mb-4">
                    If not using a DLAMI, you'll need to manually install NVIDIA drivers and the correct CUDA toolkit
                    version
                    compatible with Ollama.
                </p>

                <h3 class="text-xl font-semibold mb-4 text-green-700">3. Core Tools Installation</h3>
                <p class="text-gray-700 mb-3">Ensure these are installed (DLAMIs often include them):</p>
                <ul class="list-disc pl-5 space-y-2 text-gray-700 mb-4">
                    <li><strong>Docker & Docker Compose:</strong> For containerizing Ollama and Code-Server. Follow
                        official
                        Docker installation guides if needed.</li>
                    <li><strong>NVIDIA Container Toolkit:</strong> Allows Docker containers to access the GPU.
                        Installation
                        involves adding NVIDIA's repository and installing the `nvidia-container-toolkit` package, then
                        restarting Docker.</li>
                </ul>

                <pre><code class="language-bash"># Example NVIDIA Container Toolkit Install (Verify official docs)
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
&& curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
sudo apt-get update
sudo apt-get install -y nvidia-container-toolkit
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# Verify GPU access in Docker
sudo docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi</code></pre>

                <h3 class="text-xl font-semibold mb-4 mt-8 text-green-700">4. Deployment with Docker Compose</h3>
                <p class="text-gray-700 mb-3">
                    Create project directories (e.g., `~/ai-server/ollama_data`, `~/ai-server/code-server_data`) and a
                    `docker-compose.yml` file within `~/ai-server`:
                </p>

                <pre><code class="language-yaml"># ~/ai-server/docker-compose.yml
version: '3.8'
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "127.0.0.1:11434:11434" # IMPORTANT: Bind only to localhost for security via Nginx
    volumes:
      -./ollama_data:/root/.ollama # Persist models and data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all # Use all available GPUs
              capabilities: [gpu]
    restart: unless-stopped # Auto-start on reboot/crash

  code-server:
    image: codercom/code-server:latest
    container_name: code-server
    user: "$(id -u):$(id -g)" # Run as host user for permissions
    ports:
      - "127.0.0.1:8080:8080" # IMPORTANT: Bind only to localhost
    volumes:
      -./code-server_data:/home/coder/.local/share/code-server # Persist settings/extensions
      - $HOME:/home/coder/project # Mount home directory into Code-Server
    environment:
      - PASSWORD=YourStrongPasswordHere # CHANGE THIS!
    restart: unless-stopped # Auto-start on reboot/crash

volumes:
  ollama_data:
  code-server_data:</code></pre>

                <div class="bg-yellow-50 border-l-4 border-yellow-500 p-4 my-4">
                    <p class="text-yellow-700">
                        <strong>Security Alert:</strong> Change <code>YourStrongPasswordHere</code> to a very strong,
                        unique
                        password in the <code>docker-compose.yml</code> file.
                    </p>
                </div>

                <p class="text-gray-700 mb-3">Start the services:</p>
                <pre><code class="language-bash">cd ~/ai-server && sudo docker compose up -d</code></pre>

                <h3 class="text-xl font-semibold mb-4 mt-8 text-green-700">5. Pull Ollama Models</h3>
                <p class="text-gray-700 mb-3">Download the models you want to use:</p>
                <pre><code class="language-bash">sudo docker exec ollama ollama pull llama3:8b
sudo docker exec ollama ollama pull mistral
sudo docker exec ollama ollama pull deepseek-coder:6.7b
# Add other models as needed (e.g., phi-3, starcoder, etc.)</code></pre>
            </div>
        </section>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Secure Access with Nginx & SSL</h2>
            </div>
            <div class="p-6">
                <h3 class="text-xl font-semibold mb-4 text-green-700">Why Nginx?</h3>
                <p class="text-gray-700 mb-3">Nginx acts as a reverse proxy, providing a single, secure entry point:</p>
                <ul class="list-disc pl-5 space-y-2 text-gray-700 mb-6">
                    <li>Handles HTTPS (SSL/TLS encryption).</li>
                    <li>Hides the direct ports of Ollama and Code-Server.</li>
                    <li>Allows adding security headers centrally.</li>
                    <li>Exposes services via your domain name (or IP).</li>
                </ul>

                <h3 class="text-xl font-semibold mb-4 text-green-700">Nginx Installation & Configuration</h3>
                <p class="text-gray-700 mb-3">Install Nginx:</p>
                <pre><code class="language-bash">sudo apt update && sudo apt install nginx</code></pre>
                <p class="text-gray-700 mb-3">
                    Create an Nginx site configuration file (e.g.,
                    <code>/etc/nginx/sites-available/ai-server.conf</code>).
                    Replace <code>your_domain.com</code> with your actual domain or the EC2 instance's public IP address
                    if not
                    using a domain.
                </p>

                <pre><code class="language-nginx"># /etc/nginx/sites-available/ai-server.conf

# Redirect HTTP to HTTPS (Certbot often handles this better)
server {
    listen 80;
    listen [::]:80;
    server_name your_domain.com; # CHANGE THIS

    location /.well-known/acme-challenge/ { # For Certbot validation
        root /var/www/html;
    }

    location / {
        return 301 https://$host$request_uri;
    }
}

server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name your_domain.com; # CHANGE THIS

    # SSL Configuration - Managed by Certbot (paths added automatically)
    ssl_certificate /etc/letsencrypt/live/your_domain.com/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/your_domain.com/privkey.pem;

    # Basic Security Headers
    add_header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload" always;
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header X-XSS-Protection "0" always; # Rely on CSP if implemented

    # Hide Nginx version
    server_tokens off;

    # Proxy pass to Code-Server (at root path '/')
    location / {
        proxy_pass http://127.0.0.1:8080/; # Forward to Code-Server
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # WebSocket support - CRITICAL for Code-Server
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
    }

    # Proxy pass to Ollama API (at /ollama/ path)
    location /ollama/ {
        proxy_pass http://127.0.0.1:11434/; # Forward to Ollama
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Optional: Deny access to hidden files
    location ~ /\. {
        deny all;
    }
}</code></pre>

                <p class="text-gray-700 mb-3">Enable the site, test config, and reload Nginx:</p>
                <pre><code class="language-bash">sudo ln -sfn /etc/nginx/sites-available/ai-server.conf /etc/nginx/sites-enabled/ai-server.conf
sudo nginx -t
sudo systemctl reload nginx</code></pre>

                <h3 class="text-xl font-semibold mb-4 mt-8 text-green-700">SSL Certificates with Let's Encrypt (Certbot)
                </h3>
                <p class="text-gray-700 mb-3">If using a domain name, get a free SSL certificate:</p>
                <ol class="list-decimal pl-5 space-y-2 text-gray-700 mb-4">
                    <li>Install Certbot (snap method recommended):
                        <pre><code class="language-bash">sudo snap install core; sudo snap refresh core
sudo snap install --classic certbot
sudo ln -s /snap/bin/certbot /usr/bin/certbot</code></pre>
                    </li>
                    <li>Ensure your domain's DNS A record points to the EC2 instance's public IP.</li>
                    <li>Allow HTTPS traffic (port 443) in the EC2 Security Group.</li>
                    <li>Run Certbot:
                        <pre><code class="language-bash">sudo certbot --nginx -d your_domain.com</code></pre>
                        (Replace `your_domain.com`). Follow the prompts. Certbot will obtain the certificate and
                        automatically
                        update your Nginx configuration.
                    </li>
                    <li>Verify auto-renewal: <code>sudo certbot renew --dry-run</code>.</li>
                </ol>
            </div>
        </section>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Ensuring Reliability & Managing Costs</h2>
            </div>
            <div class="p-6">
                <h3 class="text-xl font-semibold mb-4 text-green-700">Auto-Start Services</h3>
                <p class="text-gray-700 mb-4">
                    The <code>restart: unless-stopped</code> policy in the `docker-compose.yml` file ensures that both
                    Ollama and
                    Code-Server containers will automatically restart if they crash or after the EC2 instance reboots.
                    No
                    further action is needed if using the provided Docker Compose setup.
                </p>
                <p class="text-gray-700 mb-4">
                    (If installed manually without Docker, you would need to create and enable `systemd` service files
                    for Ollama
                    and Code-Server).
                </p>

                <h3 class="text-xl font-semibold mb-4 text-green-700">Automated Shutdown (Cost Saving)</h3>
                <div class="bg-green-50 border-l-4 border-green-500 p-4 my-4">
                    <p class="text-green-700">
                        <strong>Recommended Method: AWS Lambda + EventBridge Scheduler</strong>
                    </p>
                    <p class="text-green-700">
                        This is the most secure and reliable way to automatically stop your EC2 instance during off-peak
                        hours
                        (e.g., 2 AM daily) to save costs.
                    </p>
                </div>

                <ol class="list-decimal pl-5 space-y-4 text-gray-700 mb-6">
                    <li>
                        <strong>Create IAM Policy & Role:</strong> Create an IAM policy granting `ec2:StopInstances`
                        permission
                        specifically for your EC2 instance ARN, plus basic CloudWatch Logs permissions. Create an IAM
                        role for
                        Lambda execution and attach this policy.
                    </li>
                    <li>
                        <strong>Create Lambda Function:</strong>
                        <ul class="list-disc pl-5 space-y-2 mt-2">
                            <li>Go to the AWS Lambda console, create a new function (Author from scratch, Python
                                runtime).</li>
                            <li>Assign the IAM role created above.</li>
                            <li>Use the following Python code (replace placeholders):</li>
                        </ul>
                        <pre><code class="language-python"># lambda_function.py
import boto3
import os

# Define your instance ID and region
REGION = 'us-east-1' # CHANGE if needed
INSTANCE_IDS = ['i-xxxxxxxxxxxxxxxxx'] # *** REPLACE with your actual EC2 instance ID ***

ec2 = boto3.client('ec2', region_name=REGION)

def lambda_handler(event, context):
    if not INSTANCE_IDS:
        print("Error: No instance IDs specified.")
        return {'statusCode': 400, 'body': 'Instance ID not set'}

    try:
        print(f"Attempting to stop instances: {INSTANCE_IDS} in region {REGION}")
        ec2.stop_instances(InstanceIds=INSTANCE_IDS)
        print(f"Successfully initiated stop for instances: {INSTANCE_IDS}")
        return {'statusCode': 200, 'body': f'Stop initiated for {INSTANCE_IDS}'}
    except Exception as e:
        print(f"Error stopping instances: {e}")
        return {'statusCode': 500, 'body': f'Error: {str(e)}'}
                </code></pre>
                        <ul class="list-disc pl-5 space-y-2 mt-2">
                            <li>Deploy the function.</li>
                        </ul>
                    </li>
                    <li>
                        <strong>Create EventBridge Schedule:</strong>
                        <ul class="list-disc pl-5 space-y-2 mt-2">
                            <li>Go to the Amazon EventBridge console, select "Schedules", "Create schedule".</li>
                            <li>Define a schedule using a cron expression (remember it's in UTC). Example for 2:00 AM
                                UTC daily:
                                <code>cron(0 2 ? * * *)</code>. Adjust for your desired local time shutdown.
                            </li>
                            <li>Set the target to the Lambda function created above.</li>
                            <li>Create the schedule.</li>
                        </ul>
                    </li>
                </ol>
                <p class="text-gray-700 italic">
                    Alternative (Less Recommended): A cron job on the EC2 instance itself using AWS CLI, but this
                    requires granting permissions directly to the instance.
                </p>
            </div>
        </section>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Understanding the Costs</h2>
            </div>
            <div class="p-6">
                <h3 class="text-xl font-semibold mb-4 text-green-700">Main Cost Components</h3>
                <ul class="list-disc pl-5 space-y-2 text-gray-700 mb-6">
                    <li><strong>EC2 Instance Runtime:</strong> Largest cost, depends on instance type (G5), pricing
                        model (Spot
                        recommended), and hours running. Auto-shutdown drastically reduces this.</li>
                    <li><strong>EBS Storage:</strong> Cost for the disk volume (recommend `gp3`). Charged per GB-month
                        (e.g.,
                        ~$0.08/GB-month for gp3 in us-east-1). A 250GB volume costs ~$20/month.</li>
                    <li><strong>Data Transfer OUT:</strong> First 100GB/month OUT to the internet is free. Additional
                        data costs
                        ~$0.09/GB. Usually negligible for personal use. Data IN is free.</li>
                </ul>

                <h3 class="text-xl font-semibold mb-4 text-green-700">Monthly Cost Estimate Sheet (Example: us-east-1,
                    Linux)</h3>
                <p class="text-gray-700 mb-3">
                    Assumptions: 250GB gp3 EBS volume (~$20/month), <100GB data transfer ($0). Spot prices estimated
                        ~70% off On-Demand (actual prices vary). </p>

                        <div class="overflow-x-auto">
                            <table class="w-full border-collapse">
                                <thead>
                                    <tr class="bg-green-100">
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Scenario
                                        </th>
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Instance
                                            Type</th>
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Pricing
                                            Model</th>
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Monthly
                                            Hours</th>
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Est. EC2
                                            Cost</th>
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Est. EBS
                                            Cost</th>
                                        <th class="border border-green-300 px-4 py-2 text-left text-green-800">Est.
                                            Total Monthly Cost</th>
                                    </tr>
                                </thead>
                                <tbody>
                                    <tr>
                                        <td class="border border-gray-300 px-4 py-2">Budget Small Models</td>
                                        <td class="border border-gray-300 px-4 py-2">g4dn.xlarge</td>
                                        <td class="border border-gray-300 px-4 py-2">Spot</td>
                                        <td class="border border-gray-300 px-4 py-2">220 (Auto-Shutdown)</td>
                                        <td class="border border-gray-300 px-4 py-2">~$35</td>
                                        <td class="border border-gray-300 px-4 py-2">$20</td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>~$55</strong></td>
                                    </tr>
                                    <tr class="bg-green-50 font-semibold">
                                        <td class="border border-green-300 px-4 py-2"><strong>Recommended Start</strong>
                                        </td>
                                        <td class="border border-green-300 px-4 py-2"><strong>g5.xlarge</strong></td>
                                        <td class="border border-green-300 px-4 py-2"><strong>Spot</strong></td>
                                        <td class="border border-green-300 px-4 py-2"><strong>220
                                                (Auto-Shutdown)</strong></td>
                                        <td class="border border-green-300 px-4 py-2"><strong>~$66</strong></td>
                                        <td class="border border-green-300 px-4 py-2"><strong>$20</strong></td>
                                        <td class="border border-green-300 px-4 py-2"><strong>~$86</strong></td>
                                    </tr>
                                    <tr>
                                        <td class="border border-gray-300 px-4 py-2">Recommended Start (On-Demand)</td>
                                        <td class="border border-gray-300 px-4 py-2">g5.xlarge</td>
                                        <td class="border border-gray-300 px-4 py-2">On-Demand</td>
                                        <td class="border border-gray-300 px-4 py-2">220 (Auto-Shutdown)</td>
                                        <td class="border border-gray-300 px-4 py-2">~$221</td>
                                        <td class="border border-gray-300 px-4 py-2">$20</td>
                                        <td class="border border-gray-300 px-4 py-2">~$241</td>
                                    </tr>
                                    <tr class="bg-gray-50">
                                        <td class="border border-gray-300 px-4 py-2"><strong>Recommended 70B
                                                Models</strong></td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>g5.12xlarge</strong></td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>Spot</strong></td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>220
                                                (Auto-Shutdown)</strong></td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>~$374</strong></td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>$20</strong></td>
                                        <td class="border border-gray-300 px-4 py-2"><strong>~$394</strong></td>
                                    </tr>
                                    <tr>
                                        <td class="border border-gray-300 px-4 py-2">Baseline (No Shutdown)</td>
                                        <td class="border border-gray-300 px-4 py-2">g5.xlarge</td>
                                        <td class="border border-gray-300 px-4 py-2">Spot</td>
                                        <td class="border border-gray-300 px-4 py-2">720 (24/7)</td>
                                        <td class="border border-gray-300 px-4 py-2">~$216</td>
                                        <td class="border border-gray-300 px-4 py-2">$20</td>
                                        <td class="border border-gray-300 px-4 py-2">~$236</td>
                                    </tr>
                                    <tr class="bg-gray-50">
                                        <td class="border border-gray-300 px-4 py-2">Baseline (No Shutdown, OD)</td>
                                        <td class="border border-gray-300 px-4 py-2">g5.xlarge</td>
                                        <td class="border border-gray-300 px-4 py-2">On-Demand</td>
                                        <td class="border border-gray-300 px-4 py-2">720 (24/7)</td>
                                        <td class="border border-gray-300 px-4 py-2">~$724</td>
                                        <td class="border border-gray-300 px-4 py-2">$20</td>
                                        <td class="border border-gray-300 px-4 py-2">~$744</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="bg-blue-50 border-l-4 border-blue-500 p-4 my-4">
                            <p class="text-blue-700">
                                <strong>Key Takeaway:</strong> Combining <strong>Spot Instances</strong> with
                                <strong>Automated
                                    Shutdown</strong> provides massive cost savings compared to running On-Demand 24/7.
                            </p>
                        </div>
            </div>
        </section>

        <section class="mb-12 bg-white rounded-lg shadow-md overflow-hidden">
            <div class="bg-green-600 p-6 text-white">
                <h2 class="text-2xl font-bold">Conclusion & Next Steps</h2>
            </div>
            <div class="p-6">
                <p class="text-gray-700 mb-4">
                    By following this guide, you can create a powerful, customizable, and cost-effective AI development
                    environment on AWS EC2. You gain control over your tools and data, leverage potent cloud hardware,
                    and avoid recurring SaaS fees.
                </p>
                <p class="font-semibold text-gray-700 mb-3"><strong>Next Steps to Explore:</strong></p>
                <ul class="list-disc pl-5 space-y-2 text-gray-700 mb-4">
                    <li>Experiment with different Ollama models and quantization levels.</li>
                    <li>Set up monitoring for your instance (CloudWatch).</li>
                    <li>Explore advanced Nginx features (e.g., basic auth for the Ollama endpoint).</li>
                    <li>Integrate LocalGPT alongside Ollama if you need to interact with local documents.</li>
                    <li>Automate the entire setup using Infrastructure as Code tools like Terraform or AWS
                        CloudFormation.</li>
                </ul>
            </div>
        </section>

        <div class="mt-12 p-6 bg-green-50 rounded-lg shadow">
            <h2 class="text-xl font-bold mb-4">Looking for Other Resources?</h2>
            <p class="text-gray-700 mb-4">
                Check out my other AWS research pages:
            </p>
            <div class="flex flex-wrap gap-4">
                <a href="../deepseek/"
                    class="inline-block bg-white border border-green-200 px-4 py-2 rounded hover:bg-green-100 transition-colors text-green-700">
                    DeepSeek AI Config
                </a>
                <a href="../flowith/"
                    class="inline-block bg-white border border-green-200 px-4 py-2 rounded hover:bg-green-100 transition-colors text-green-700">
                    Flowith AI Deployment
                </a>
                <a href="../grok/"
                    class="inline-block bg-white border border-green-200 px-4 py-2 rounded hover:bg-green-100 transition-colors text-green-700">
                    Grok AI Configuration
                </a>
                <a href="../manus/"
                    class="inline-block bg-white border border-green-200 px-4 py-2 rounded hover:bg-green-100 transition-colors text-green-700">
                    Manus AI Implementation
                </a>
            </div>
        </div>
    </main>

    <footer class="bg-gray-800 text-white py-8">
        <div class="container mx-auto px-4 max-w-4xl">
            <div class="flex flex-col md:flex-row justify-between items-center">
                <p class="mb-4 md:mb-0">&copy; 2025 Abdur-Rahmān Bilāl. All rights reserved.</p>
                <div class="flex gap-4">
                    <a href="/" class="text-white hover:text-green-400">Home</a>
                    <a href="https://github.com/aramb-dev" class="text-white hover:text-green-400">GitHub</a>
                    <a href="mailto:aramb@aramb.dev" class="text-white hover:text-green-400">Contact</a>
                </div>
            </div>
        </div>
    </footer>

    <!-- Script for copy functionality -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            // Find all pre tags
            const preTags = document.querySelectorAll('pre');

            // Add copy button to each pre tag
            preTags.forEach(function (preTag, index) {
                // Create a wrapper div with relative positioning
                const wrapper = document.createElement('div');
                wrapper.className = 'relative';

                // Create the copy button
                const copyButton = document.createElement('button');
                copyButton.id = `copy-script-button-${index}`;
                copyButton.className = 'absolute top-2 right-2 bg-green-600 text-white rounded p-1 text-xs hover:bg-green-700';
                copyButton.textContent = 'Copy Script';

                // Get the parent of the pre tag
                const parent = preTag.parentNode;

                // Replace pre with wrapper
                parent.replaceChild(wrapper, preTag);

                // Add button and pre to wrapper
                wrapper.appendChild(copyButton);
                wrapper.appendChild(preTag);

                // Add click event to copy button
                copyButton.addEventListener('click', function () {
                    const scriptContent = preTag.textContent;
                    if (scriptContent) {
                        navigator.clipboard.writeText(scriptContent)
                            .then(() => {
                                // Visual feedback that copying worked
                                const originalText = this.textContent;
                                this.textContent = 'Copied!';
                                setTimeout(() => {
                                    this.textContent = originalText;
                                }, 2000);
                            })
                            .catch(err => {
                                console.error('Failed to copy: ', err);
                            });
                    }
                });
            });
        });
    </script>
</body>

</html>